import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}
import org.apache.spark.mllib.linalg.Vectors

val data = sc.textFile("hdfs://cshadoop1.utdallas.edu//hw3spring/itemusermat")
val parsedData = data.map(s => Vectors.dense(s.split(' ').drop(1).map(_.toDouble))).cache()

//KMeans for clustering data
val numberOfClusters = 10
val numberOfIterations = 20
val clusters = KMeans.train(parsedData, numberOfClusters, numberOfIterations)

val prediction = data.map{ line =>	
val parts = line.split(' ')	
(parts(0),clusters.predict(Vectors.dense(parts.tail.map(_.toDouble))))
}

val movie = sc.textFile("/hw3spring/movies.dat")
val  moviesData = movie.map{ line=>	
val parts = line.split("::")
(parts(0),(parts(1)+" , "+parts(2)))
}

val joinedData = prediction.join(moviesData)
val shuffledData= joinedData.map(p=>(p._2._1,(p._1,p._2._2)))

val gData = shuffledData.groupByKey()

val ans = gData.map(p=>(p._1,p._2.toList))

val finalAnswer = ans.map(p=>(p._1,p._2.take(5)))

finalAnswer.collect

println("Cluster Id , List of first 5 Movies in cluster")
finalAnswer.foreach(p=>println("Cluster id - "+ p._1+" --> "+p._2.mkString(":::")))